Broom memoryLimit: 1024 * 1024 * 1024 * 4.

import main: \*.
import
  Library/Test: 'describe'
.

#:language XFrozen

describe TokenAnalyser do: {:it:let
  it exists do: {
    { TokenAnalyser. } should not raiseError: String.
  }.

  let[Reflect thisContext] lexer { Lexer }.
  let[Reflect thisContext] analyser { NGramHelper }.
  let[Reflect thisContext] glblctx { Reflect thisContext }.
  let[Reflect thisContext] tokenmap { File new: 'tokenMap', open: 'w+' }.
  let[Reflect thisContext] limit { 1000 }.
  let[Reflect thisContext] ngram-N { 3 }.

  it can tokenise and calculate NGram do: {:thisTest
    var allTargetTexts is (Generator from: 2 to: limit) fmap: (\:x File new: 'samples/sample$$x', read).
    # (
    #   File
    #     list: 'samples',
    #     filter: (\:_:descr (descr @ 'type' = 'file') & (descr @ 'file' startsWith: 'sample')),
    #     take: 1,
    #     asGenerator fmap: \:descr (File new: 'samples/' + (descr @ 'file'), read)
    # ).

    lexer collate: (File new: 'collation', read split: '\n', fmap: {:x ^x split: ' ' max: 1. }).

    thisTest tests: '${{ngram-N}}$-gram'.
    thisTest progressesUpTo: limit - ngram-N + 1.
    var res is analyser
      ngramCountWithN: ngram-N
      onTexts: allTargetTexts
      withLexer: lexer
      pre: {:i thisTest progress: i.}
      post: {:i thisTest show.}.
    var tokenTable is res last.
    var ngramCount is res head.
    var ngramCountVec is ngramCount extractDimension: ngram-N - 1 atIndex: 0, multiplicativeInverse.
    ngramCountVec writeInto: 'countinv'.
    var ngramProb is ngramCount multiply: ngramCountVec throughDimension: ngram-N - 1.

    ngramCount writeInto: 'count'.
    ngramProb writeInto: 'prob'.
    tokenTable each: {:k:v
      tokenmap write: '$$k : $$v \n'.
    }.
    tokenmap close.
  }.

}.
